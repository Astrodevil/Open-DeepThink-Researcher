{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio gradio aiohttp openai exa_py\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import gradio as gr\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from exa_py import Exa\n",
        "\n",
        "# Configuration\n",
        "NEBIUS_API_KEY = \"your_nebius_api_key\" # Replace with your Nebius API key\n",
        "EXA_API_KEY = \"your_exa_api_key\" # Replace with your EXA API key\n",
        "\n",
        "exa = Exa(EXA_API_KEY)\n",
        "client = OpenAI(base_url=\"https://api.studio.nebius.ai/v1/\", api_key=NEBIUS_API_KEY)\n",
        "\n",
        "DEFAULT_MODEL = \"deepseek-ai/DeepSeek-R1-fast\"\n",
        "\n",
        "# Functions\n",
        "async def call_nebius_async(session, messages, model=DEFAULT_MODEL):\n",
        "    completion = client.chat.completions.create(model=model, messages=messages, temperature=0.6)\n",
        "    if completion.choices:\n",
        "        return completion.choices[0].message.content\n",
        "    return None\n",
        "\n",
        "async def generate_search_queries_async(session, user_query):\n",
        "    prompt = (\n",
        "        \"You are an expert research assistant. Generate exactly four distinct search queries \"\n",
        "        \"that would help gather complete information about this topic. Format your response \"\n",
        "        \"as a Python list of strings. Example format: ['query1', 'query2', 'query3', 'query4']. \"\n",
        "        f\"Query: {user_query}\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and precise research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    response = await call_nebius_async(session, messages)\n",
        "    if response:\n",
        "        try:\n",
        "            cleaned_response = response.strip()\n",
        "            import re\n",
        "            if not cleaned_response.startswith('['):\n",
        "                list_match = re.search(r'\\[(.*?)\\]', cleaned_response)\n",
        "                if list_match:\n",
        "                    cleaned_response = list_match.group(0)\n",
        "            search_queries = eval(cleaned_response)\n",
        "            if isinstance(search_queries, list):\n",
        "                return search_queries\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing search queries: {str(e)}\")\n",
        "    return []\n",
        "\n",
        "async def perform_search_async(query):\n",
        "    try:\n",
        "        response = await asyncio.to_thread(exa.search_and_contents, query)\n",
        "        if response and response.results:\n",
        "            return [(result.url, result.text) for result in response.results if result.text]\n",
        "    except Exception as e:\n",
        "        print(f\"Exa API search failed for query '{query}': {e}\")\n",
        "    return []\n",
        "\n",
        "async def is_page_useful_async(session, user_query, page_text):\n",
        "    prompt = (\n",
        "        \"You are a critical research evaluator. Given the user's query and the content of a webpage, \"\n",
        "        \"determine if the webpage contains information that is useful for addressing the query. \"\n",
        "        \"Respond with exactly one word: 'Yes' if the page is useful, or 'No' if it is not. Do not include any extra text.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a strict and concise evaluator of research relevance.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nWebpage Content (first 2000 characters):\\n{page_text[:2000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_nebius_async(session, messages)\n",
        "    if response:\n",
        "        answer = response.strip()\n",
        "        if answer in [\"Yes\", \"No\"]:\n",
        "            return answer\n",
        "        elif \"Yes\" in answer:\n",
        "            return \"Yes\"\n",
        "        elif \"No\" in answer:\n",
        "            return \"No\"\n",
        "    return \"No\"\n",
        "\n",
        "async def extract_relevant_context_async(session, user_query, search_query, page_text):\n",
        "    prompt = (\n",
        "        \"You are an expert information extractor. Given the user's query, the search query that led to this page, \"\n",
        "        \"and the webpage content, extract all pieces of information that are useful for answering the user's query. \"\n",
        "        \"Return only the relevant context as plain text without extra commentary.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert in extracting and summarizing relevant information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nSearch Query: {search_query}\\n\\nWebpage Content (first 2000 characters):\\n{page_text[:2000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_nebius_async(session, messages)\n",
        "    return response.strip() if response else \"\"\n",
        "\n",
        "async def get_new_search_queries_async(session, user_query, previous_search_queries, all_contexts):\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"You are an analytical research assistant. Based on the original query, the search queries performed so far, \"\n",
        "        \"and the extracted contexts from webpages, decide if further research is needed. \"\n",
        "        \"If further research is needed, provide up to four new search queries as a Python list (for example, \"\n",
        "        \"['new query1', 'new query2']). If you believe no further research is needed, respond with exactly <done>.\"\n",
        "        \"\\nOutput only a Python list or the token <done> without any extra text.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a systematic research planner.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nPrevious Search Queries: {previous_search_queries}\\n\\nExtracted Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_nebius_async(session, messages)\n",
        "    if response:\n",
        "        cleaned = response.strip()\n",
        "        if cleaned == \"<done>\":\n",
        "            return \"<done>\"\n",
        "        try:\n",
        "            new_queries = eval(cleaned)\n",
        "            if isinstance(new_queries, list):\n",
        "                return new_queries\n",
        "            print(\"LLM did not return a list for new search queries. Response:\", response)\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing new search queries:\", e, \"\\nResponse:\", response)\n",
        "    return []\n",
        "\n",
        "async def generate_final_report_async(session, user_query, all_contexts):\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"You are an expert researcher and report writer. Based on the gathered contexts below and the original query, \"\n",
        "        \"write a complete, well-structured, and detailed report that addresses the query thoroughly. \"\n",
        "        \"Include all useful insights and conclusions without extra commentary.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a skilled report writer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nGathered Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    return await call_nebius_async(session, messages)\n",
        "\n",
        "async def process_link(session, link_and_content, user_query, search_query, log):\n",
        "    link, page_text = link_and_content\n",
        "    log.append(f\"Processing content from: {link}\")\n",
        "    if not page_text:\n",
        "        log.append(f\"No content received from Exa for: {link}\")\n",
        "        return None\n",
        "    usefulness = await is_page_useful_async(session, user_query, page_text)\n",
        "    log.append(f\"Page usefulness for {link}: {usefulness}\")\n",
        "    if usefulness == \"Yes\":\n",
        "        context = await extract_relevant_context_async(session, user_query, search_query, page_text)\n",
        "        if context:\n",
        "            log.append(f\"Extracted context from {link} (first 200 chars): {context[:200]}\")\n",
        "            return context\n",
        "    return None\n",
        "\n",
        "async def async_research(user_query, iteration_limit):\n",
        "    aggregated_contexts = []\n",
        "    all_search_queries = []\n",
        "    log_messages = []\n",
        "    iteration = 0\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        log_messages.append(\"Generating initial search queries...\")\n",
        "        new_search_queries = await generate_search_queries_async(session, user_query)\n",
        "        if not new_search_queries:\n",
        "            log_messages.append(\"No search queries were generated by the LLM. Exiting.\")\n",
        "            return \"No search queries were generated by the LLM. Exiting.\", \"\\n\".join(log_messages)\n",
        "        all_search_queries.extend(new_search_queries)\n",
        "        log_messages.append(f\"Initial search queries: {new_search_queries}\")\n",
        "\n",
        "        while iteration < iteration_limit:\n",
        "            log_messages.append(f\"\\n=== Iteration {iteration + 1} ===\")\n",
        "            iteration_contexts = []\n",
        "            search_tasks = [perform_search_async(query) for query in new_search_queries]\n",
        "            search_results = await asyncio.gather(*search_tasks)\n",
        "\n",
        "            unique_links = {}\n",
        "            for idx, results in enumerate(search_results):\n",
        "                query_used = new_search_queries[idx]\n",
        "                for url, content in results:\n",
        "                    if url not in unique_links:\n",
        "                        unique_links[url] = (url, content), query_used\n",
        "\n",
        "            log_messages.append(f\"Aggregated {len(unique_links)} unique links from this iteration.\")\n",
        "            link_tasks = [\n",
        "                process_link(session, link_data[0], user_query, link_data[1], log_messages)\n",
        "                for link_data in unique_links.values()\n",
        "            ]\n",
        "            link_results = await asyncio.gather(*link_tasks)\n",
        "            for res in link_results:\n",
        "                if res:\n",
        "                    iteration_contexts.append(res)\n",
        "\n",
        "            if iteration_contexts:\n",
        "                aggregated_contexts.extend(iteration_contexts)\n",
        "                log_messages.append(f\"Found {len(iteration_contexts)} useful contexts in this iteration.\")\n",
        "            else:\n",
        "                log_messages.append(\"No useful contexts were found in this iteration.\")\n",
        "\n",
        "            new_search_queries = await get_new_search_queries_async(session, user_query, all_search_queries, aggregated_contexts)\n",
        "            if new_search_queries == \"<done>\":\n",
        "                log_messages.append(\"LLM indicated that no further research is needed.\")\n",
        "                break\n",
        "            elif new_search_queries:\n",
        "                log_messages.append(f\"LLM provided new search queries: {new_search_queries}\")\n",
        "                all_search_queries.extend(new_search_queries)\n",
        "            else:\n",
        "                log_messages.append(\"LLM did not provide any new search queries. Ending the loop.\")\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        log_messages.append(\"\\nGenerating final report...\")\n",
        "        final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
        "        return final_report, \"\\n\".join(log_messages)\n",
        "\n",
        "def run_research(user_query, iteration_limit=10):\n",
        "    return asyncio.run(async_research(user_query, iteration_limit))\n",
        "\n",
        "# Gradio UI Setup\n",
        "def gradio_run(user_query, iteration_limit):\n",
        "    try:\n",
        "        final_report, logs = run_research(user_query, int(iteration_limit))\n",
        "        return final_report, logs\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\", \"\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_run,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, label=\"Research Query/Topic\", placeholder=\"Enter your research query here...\"),\n",
        "        gr.Slider(minimum=1, maximum=10, step=1, value=5, label=\"Max Iterations\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Final Report\", lines=10),\n",
        "        gr.Textbox(label=\"Intermediate Steps Log\", lines=5),\n",
        "    ],\n",
        "    title=\"OpenDeepSeek-Researcher\",\n",
        "    description=\"Enter your query and a maximum iteration count to generate a detailed report. The log will show the steps taken during the research process.\",\n",
        "\n",
        "    css=\"\"\"\n",
        "        body {\n",
        "            font-family: 'Arial', sans-serif;\n",
        "        }\n",
        "        .gradio-container {\n",
        "            max-width: 900px;\n",
        "            margin: 0 auto;\n",
        "            padding: 20px;\n",
        "            border-radius: 8px;\n",
        "            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n",
        "        }\n",
        "        .gr-box {\n",
        "            border-radius: 8px;\n",
        "        }\n",
        "        .gr-button {\n",
        "            background-color: #4CAF50; /* Example color */\n",
        "            color: white;\n",
        "            border: none;\n",
        "            padding: 10px 20px;\n",
        "            text-align: center;\n",
        "            text-decoration: none;\n",
        "            display: inline-block;\n",
        "            font-size: 16px;\n",
        "            border-radius: 5px;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        [\"What are the benefits of intermittent fasting?\", 5],\n",
        "        [\"Explain the history of the internet.\", 3],\n",
        "        [\"Compare Nebius Studio with other Providers.\", 2],\n",
        "        [\"Cheapest FLUX model providers.\", 4]\n",
        "    ]\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "46Q5XpapDJZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c11ec111-e402-4e14-91d6-4484d33e0b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.16.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.12)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: exa_py in /usr/local/lib/python3.11/dist-packages (1.8.8)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from exa_py) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->exa_py) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->exa_py) (2.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://246572fe2afc0eef49.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://246572fe2afc0eef49.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}